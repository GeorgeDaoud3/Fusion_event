# Fusion_event


## ğŸ” Problem

Autonomous vehicles rely on sensors like LiDAR and cameras to perceive their environment. Each sensor has strengths and limitations:

ğŸ”¹ LiDAR provides accurate 3D data but lacks color information.

ğŸ”¹ Cameras capture rich visuals but are sensitive to lighting conditions.


Individually, these sensors can be noisy or miss key details due to occlusions from other road agents. However, by fusing data from multiple sensors and vehicles, we can create a more reliable, comprehensive view of the scene, improving safety and awareness.

## ğŸš¦ Scenario

Two self-driving cars are approaching an intersection, each equipped with:


âœ… 3D LiDAR

âœ… Camera

The environment includes vehicles, pedestrians, and cyclists, some of whom may block each vehicleâ€™s view. By communicating and sharing sensor data, the vehicles can collaborate to overcome occlusions and enhance situational understanding.

![scene](/images/scene.png)

## ğŸ¯ Goal

Process the raw camera and LiDAR data from both vehicles to:


ğŸ”¹ Generate individual object detection outputs for each car.

ğŸ”¹ Fuse the data to build a shared perception of the scene.

ğŸ”¹ Enhance visibility by addressing sensor occlusions and inconsistencies.

ğŸ”¹ Output a visual representation showing detected agents from both perspectives.
